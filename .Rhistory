#nrow(expand.grid(list(alpha = alphas, lambda = lambdas) ))/1
end_time1 <- Sys.time()
print(difftime(init_time1,end_time1))
View(elastic_net_reg$results)
View(elastic_net_reg$results)
elastic_net_reg <- train(
x = as.matrix(filter_train %>% select(-ytarget) ),
y = filter_train[,ytarget],
method = "glmnet",
trControl = ts_trControl,
tuneGrid = elastic.net.gridFilter,
metric = 'RMSE',
maximize = FALSE, tuneLength = 5      )
View(elastic_net_reg$results)
#nrow(expand.grid(list(alpha = alphas, lambda = lambdas) ))/1
end_time1 <- Sys.time()
init_time1 <- Sys.time()
elastic_net_reg <- train(
x = as.matrix(filter_train %>% select(-ytarget) ),
y = filter_train[,ytarget],
method = "glmnet",
trControl = ts_trControl,
tuneGrid = elastic.net.gridFilter,
metric = 'RMSE',
maximize = FALSE, tuneLength = 5      )
View(elastic_net_reg$results)
#nrow(expand.grid(list(alpha = alphas, lambda = lambdas) ))/1
end_time1 <- Sys.time()
init_time1 <- Sys.time()
elastic_net_reg <- train(
x = as.matrix(filter_train %>% select(-ytarget) ),
y = filter_train[,ytarget],
method = "glmnet",
trControl = ts_trControl,
tuneGrid = elastic.net.gridFilter,
metric = 'RMSE',
maximize = FALSE, tuneLength = 5      )
#View(elastic_net_reg$results)
#nrow(expand.grid(list(alpha = alphas, lambda = lambdas) ))/1
end_time1 <- Sys.time()
print(difftime(init_time1,end_time1))
View(elastic_net_reg$results)
init_time1 <- Sys.time()
elastic_net_reg <- train(
x = as.matrix(filter_train %>% select(-ytarget) ),
y = filter_train[,ytarget],
method = "glmnet",
trControl = ts_trControl,
tuneGrid = elastic.net.gridFilter,
metric = 'RMSE',
maximize = FALSE, tuneLength = 20     )
#View(elastic_net_reg$results)
#nrow(expand.grid(list(alpha = alphas, lambda = lambdas) ))/1
end_time1 <- Sys.time()
print(difftime(init_time1,end_time1))
View(elastic_net_reg$results)
init_time1 <- Sys.time()
elastic_net_reg <- train(
x = as.matrix(filter_train %>% select(-ytarget) ),
y = filter_train[,ytarget],
method = "glmnet",
trControl = ts_trControl,
tuneGrid = elastic.net.gridFilter,
metric = 'RMSE',
maximize = FALSE   )
#View(elastic_net_reg$results)
#nrow(expand.grid(list(alpha = alphas, lambda = lambdas) ))/1
end_time1 <- Sys.time()
print(difftime(init_time1,end_time1))
View(elastic_net_reg$results)
elastic.net.grid <- expand.grid(list(alpha = alphas, lambda = lambdas) )
# Define train and test horizon
H <- 240
horizonfull <- 1:H
last_time <- min(DATA$DateTime) + (3600 * (18*24))
FSTUDYDAYS <- seq(from = last_time,
to = max(DATA$DateTime) - (3600 * (H)),
by = 3600 * 24)
N <- length(FSTUDYDAYS)
IDTEST <- list()
for (i.hm in 1:N) {
IDTEST[[i.hm]] <- which(DATA$DateTime >= FSTUDYDAYS[i.hm] + 1 * 3600 & DATA$DateTime <= FSTUDYDAYS[i.hm] + H * 3600 & FSTUDYDAYS[i.hm] == DATA$forecast_origin) # == FSTUDYDAYS[i.hm] == DATA$forecast_origin restricts to most recent known weather forecasts
# View(DATA[which(DATA$DateTime >= FSTUDYDAYS[239+1] + 1 * 3600 & DATA$DateTime <= FSTUDYDAYS[239+1] + H * 3600),])
}
# verify length of each subset
mm <- list()
for (i.hm in 1:N) mm[[i.hm]] <- length(IDTEST[[i.hm]])
for (v in IDTEST){
cat(length(v),'\n')
}
# define models to estimate
# "true", "bench", "GAM", "AR", "hw"
model.names <- c("elasticNet","AR", "true", "bench")
M <- length(model.names)
ytarget <- yt_name
# for (i.m in model.names)
FORECASTS <- array(, dim = c(N, H, M))
dimnames(FORECASTS) <- list(format(FSTUDYDAYS, "%Y-%m-%d"), paste("h_", 1:H, sep = ""), model.names)
cat('dimension of result matrix*******************************************************\n')
dim(FORECASTS)
#define ls of training time
ls_train_time <- list()
# run model estimation
S <- 24
for (i.m in seq_along(model.names)) {
mname <- model.names[i.m]
init_time <- Sys.time()
cat('\\\\\\\\-- begin model', mname, '........i.m*******************************************//////////\n',sep = ' ')
if (mname %in% c("true", "bench", "GAM", "xgb", "elasticNet")) {
LAGS <- S * c(1:14, 21, 28)
horizonc <- unique(c(0, findInterval(LAGS, 1:H)))
} else { # AR
horizonc <- c(0, H)
}
# define hyperparamters once
if (mname == 'xgb'){
lgb.grid <- base::expand.grid(
list(
boosting= c("gbdt","dart"),
obj = c("mse"),
num_leaves = seq(20, 40, 1),
max_depth = seq(2,10,1),
min_sum_hessian_in_leaf = c(4,6),
bagging_fraction = c(0.8),
learning_rate = seq(0.01, 0.2, 0.01),
num_iterations = seq(40, 65, 3),
lambda_l1 = seq(0.01, 0.1, 0.01),
lambda_l2 = seq(0.01, 0.1, 0.01)
))
}
#dim(lgb.grid)#96768000
# define horizon separation
reest <- 20
FSTUDYSEQid <- unique(c(seq(0, length(FSTUDYDAYS), by = reest), length(FSTUDYDAYS)))
FSTUDYSEQ <- list()
for (i.seq in 1:(length(FSTUDYSEQid) - 1)) FSTUDYSEQ[[i.seq]] <- FSTUDYDAYS[c((FSTUDYSEQid[i.seq] + 1), FSTUDYSEQid[i.seq + 1])]
Nsplitlen <- length(FSTUDYSEQ)
cat('# of horizon separation *N*', Nsplitlen, 'of model', mname, '*******************************************\n',sep = ' ')
# model specific data preparation for the forecasting study [model dependent]:
if(mname %in% c("GAM", "xgb", "elasticNet")){
#DATA$DateTime
vec <- as.integer(DATA$DateTime)
subs <- match(unique(vec), vec)
TMPDATA <- bind_cols(DateTime = DATA$DateTime[subs],
shiftDST(DATA[subs,ytarget],
summer = DATA$SummerTime[subs],
clag = LAGS) )
#bind_cols(EDAT[[zone]][, "DateTime"], as.data.table(shift(as.data.frame(EDAT[[zone]][, 1:2 + 1]), LAGS, give.names = TRUE)))
dff <- as.data.frame(base::as.factor(DATA$HoD))
names(dff) <- c("HoD")
MHoW <- as.matrix(sparse.model.matrix(~.-1, data = dff))
dfDoW <- as.data.frame(base::as.factor(DATA$DoW))
names(dfDoW) <- c("DoW")
MDoW <- as.matrix(sparse.model.matrix(~.-1,data = dfDoW))
dfMoY <- as.data.frame(base::as.factor(DATA$MoY))
names(dfMoY) <- c("MoY")
MMoY <- as.matrix(sparse.model.matrix(~.-1,data = dfMoY))
dfQoY <- as.data.frame(base::as.factor(DATA$QoY))
names(dfQoY) <- c("QoY")
MQoY <- as.matrix(sparse.model.matrix(~.-1,data = dfQoY))
weekend_mx <- matrix(DATA$weekend, nrow = nrow(DATA), ncol = 1 )
colnames(weekend_mx) <- c("weekend")
all_dummys <- cbind(MHoW, MDoW, MMoY,MQoY, weekend_mx )
cat("interaction features****************************************************\n")
paste_hod <- paste("HoD",0:22, sep="")
paste_dow <- paste("DoW",1:6, sep="")
paste_moy <- paste("MoY",1:11, sep="")
paste_qoy <- paste("QoY",1:3, sep="")
paste_seasonality <- as.vector( sapply(c("HoD","DoW", "MoY", "DoY", "WoY", "DoM", "QoY"),
function(x) c(paste(x,'_sin',sep = ''), paste(x,'_cos',sep = '')   )) )
col_comb_features <- c(paste_dow, paste_hod, paste_moy, paste_qoy, 'weekend' )
all_comb <- combn(col_comb_features, 2)
ls_ds_interaction <- list()
for (ncomb in 1:ncol(all_comb)){
#View(all_comb)
pt_1 <- all_comb[1,ncomb]
pt_2 <- all_comb[2,ncomb]
if ( substr(pt_1, start=1, stop=3) != substr(pt_2, start=1, stop=3) ){
ls_ds_interaction[[ncomb]] <- paste(pt_1,pt_2,sep = 'and')
}
}
ls_ds_interaction <- Filter(Negate(is.null), ls_ds_interaction)
ls_ds_interaction <- str_split(ls_ds_interaction,'and')
mx_interaction <- matrix(,ncol = 2, nrow = length(ls_ds_interaction) )
for (i_comb in 1:length(ls_ds_interaction)) mx_interaction[i_comb,] <- c(ls_ds_interaction[[i_comb]][1],ls_ds_interaction[[i_comb]][2])
ds_interaction <- setNames(data.frame(mx_interaction), c('x','y') )
tmp_ds_transf <- list()
tmp_ds_transf[[1]] <- all_dummys[,col_comb_features]
ls_ds_transf <- list()
for (id_ds in length(tmp_ds_transf)){
ds_tmp_orig <- tmp_ds_transf[[id_ds]]
comb_vector <- ds_interaction
name_vector_comb <- sub("\\.var", ".", do.call(paste, c(comb_vector, sep=".")))
formula_evaluate <- sprintf("transform(ds_tmp_orig, %s = %s*%s)", name_vector_comb,
comb_vector[,1], comb_vector[,2])
colnames(ds_tmp_orig)
for(i in seq_along(formula_evaluate)) ds_tmp_orig <- eval(parse(text = formula_evaluate[i]))
final_transf_ds <- ds_tmp_orig[,c(name_vector_comb)]
ls_ds_transf[[id_ds]] <- final_transf_ds
}
#define features to use
features_interaction <- colnames(ls_ds_transf[[1]])
features_x <- c(paste_hod, paste_dow, paste_moy,
paste_qoy, paste("x_lag_",S * c(1:14, 21, 28), sep=""),
paste_seasonality, features_interaction,
'weekend', "SummerTime")
TMPDATA <- cbind(TMPDATA, all_dummys[subs, -unlist(list(ncol(all_dummys)))], ls_ds_transf[[1]][subs,])
FDATA <- dplyr::full_join(DATA, TMPDATA, by = c("DateTime")) %>% arrange(DateTime, horizon)
} else {
FDATA <- DATA
}
DATATEST <- FDATA[unlist(IDTEST), ]
# begin loop by rows 'N'
#i.N <- 1
cat('------||| begin iteration over each block of rows **N**, FSTUDYSEQ..rows..i.N........|||------\n')
for (i.N in seq_along(FSTUDYSEQ)) {
cat('**--part N#', i.N, 'of model', mname, '--**\n',sep = ' ')
# define id of reestimation
seqid <- ((FSTUDYSEQid[i.N] + 1):FSTUDYSEQid[i.N + 1])
# id row....
#  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20
HORIZON <- list()
for (i.hl in seq_along(horizonc)[-1]) HORIZON[[i.hl - 1]] <- (horizonc[i.hl - 1] + 1):horizonc[i.hl]
#HORIZON, BLOCK OF 24 DAYS, 10days
#[[9]]
#[1] 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216
#[[10]]
#[1] 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240
# begin reestimation
for (i.hl in seq_along(HORIZON)) {
# cat('HORIZON........24,48....240..ahead.\n')
#i.hl <- 1
cat('+++++** reestimation #', i.hl, 'of model', mname, 'from the part #', i.N,'***+++\n',sep = ' ')
# define max and min horizons
hmin <- head(HORIZON[[i.hl]], 1)
hmax <- tail(HORIZON[[i.hl]], 1)
idt <- FDATA$DateTime <= FSTUDYSEQ[[i.N]][1] & FDATA$horizon >= hmin & FDATA$horizon <= hmax & !is.na(FDATA$horizon)
#View(FDATA[FDATA$DateTime <= FSTUDYSEQ[[i.N]][1],])
# View(FDATA[FDATA$DateTime <= FSTUDYSEQ[[i.N]][1] & FDATA$horizon >= hmin & FDATA$horizon <= hmax & !is.na(FDATA$horizon),])
DATAtrain <- FDATA[idt, ]
idtestl <- list()
for (i.hm in seqid) {
idtestl[[i.hm]] <- which(DATATEST$DateTime >= FSTUDYDAYS[i.hm] + hmin * 3600 & DATATEST$DateTime <= FSTUDYDAYS[i.hm] + hmax * 3600 & DATATEST$horizon >= hmin & DATATEST$horizon <= hmax & FSTUDYDAYS[i.hm] == DATATEST$forecast_origin)
}
idtest <- unlist(idtestl)
length(idtest)
# form dataset and correct dataset
ls_ds_each_row_date <- list()
for (i.hm in seqid) {
if (length(idtestl[[i.hm]])>0){
ids_sample <- idtestl[[i.hm]]
tmp_sample_ds <- DATATEST[ids_sample, ]
ls_ds_each_row_date[[i.hm]] <- tmp_sample_ds
} else {
# create empty matrix/vector
empty_ds <- as.data.frame(matrix(nrow = hmax-hmin+1 ,ncol = ncol(DATATEST) ) )
colnames(empty_ds) <- colnames(DATATEST)
empty_ds$DateTime <- seq(FSTUDYDAYS[i.hm] + hmin * 3600, FSTUDYDAYS[i.hm] + hmax * 3600, by = 3600)
empty_ds$forecast_origin <- FSTUDYDAYS[i.hm]
empty_ds$horizon <- seq(hmin, hmax, by = 1)
ls_ds_each_row_date[[i.hm]] <- empty_ds
}
}
# combine subsets
DATAtest <- Reduce(function(x,y) bind_rows(x,y), ls_ds_each_row_date) %>%
arrange(DateTime, horizon)
ytarget <- paste(zone, "_Load_Actual", sep = "")
DATAtest <- DATAtest %>%
arrange(horizon, DateTime)
if (mname == "GAM") {
act_lags <- LAGS[LAGS >= hmax]
formstr <- paste(ytarget, "~",paste(paste("LT_Load_Actual_lag_",act_lags, sep=""),collapse="+"),"+",paste(paste("HoD",0:23, sep=""),collapse="+"),sep = "") # + ti(TTT,k=6, bs='cs')
form <- as.formula(formstr)
mod <- lm(form, data = DATAtrain)#bam(form, data = DATAtrain, select = TRUE, gamma = log(dim(DATA)[1]) / 2, discrete = TRUE)
print(summary(mod))
pred <- t(matrix(predict(mod, newdata = DATAtest), nrow = length(HORIZON[[i.hl]]), byrow = TRUE))
} # GAM
if (mname == "xgb") {
act_lags <- LAGS[LAGS >= hmax]
formula_str <- paste(
paste(ytarget,' ~ ',sep = ''),
paste( paste_dow, collapse=' + '),' + ',
paste( paste_moy, collapse=' + '),' + ',
paste( paste_hod, collapse=' + '),' + ',
paste( paste_qoy, collapse=' + '),' + ',
paste( paste_seasonality, collapse=' + '),' + ',
paste( features_interaction, collapse=' + '),' + ',
paste( paste("x_lag_",S * c(1:14, 21, 28), sep = '', collapse=' + '), '+ weekend+ SummerTime'), sep = '')
#DATAtrain[, c(features_x[1:565])]
#length(features_x[1:566])
#colnames(DATAtrain)
filter_train <- DATAtrain[, c(ytarget, features_x)] %>%
replace(is.na(.), 0)
filter_test <- DATAtest[, c(features_x)] %>%
replace(is.na(.), 0)
cat('estimating lgb.....\n')
set.seed(1)
samp <- sample(1:nrow(lgb.grid ), 15)
lgb.gridFilter <- lgb.grid [samp,]
# create datasets
lgb.test <- lgb.Dataset(data = as.matrix(filter_test),
label = DATAtest[,ytarget] )
lgb.train <- lgb.Dataset(data = as.matrix(filter_train %>% select(-ytarget) %>% head(nrow(filter_train)-20) ),
label = head(filter_train[,ytarget], nrow(filter_train)-20) )
lgb.valid <- lgb.Dataset(data = as.matrix(filter_train %>% select(-ytarget) %>% tail(20) ),
label = head(filter_train[,ytarget], tail(20) ) )
#6+"jjj"
recent_time <- Sys.time()
ls_models <- list()
ls_bst_score <- list()
for (hyper_combi in 1:nrow(lgb.gridFilter)){
ls_params <- as.list(data.frame(lgb.gridFilter[hyper_combi,]) )
obj <- as.character(ls_params$obj)
ls_params <- within(ls_params, rm(obj))
watchlist <- list(validation = lgb.valid)
lgb_alg <- lgb.train(params = ls_params,obj = obj,
data = lgb.train, valids = watchlist,
early_stopping_rounds = 70,verbose = 1,
eval_freq = 10, force_col_wise=TRUE,
nthread = 5)
ls_models[[hyper_combi]] <- lgb_alg
ls_bst_score[[hyper_combi]] <- lgb_alg$best_score
}
cat('finish lgb.....\n')
last_time <- Sys.time()
diff_mode_iter <- difftime(recent_time,last_time)
best_tune_model <- ls_models[[which.min(unlist(ls_bst_score))]]
test_sparse  = Matrix(as.matrix(filter_test), sparse=TRUE)
cat('test has: ',dim(filter_test),'\n')
cat(length(HORIZON[[i.hl]]),'----' ,length(seqid),'\n')
pred <- t(matrix(predict(best_tune_model, data = test_sparse),
nrow = length(HORIZON[[i.hl]]), ncol= length(seqid), byrow = TRUE))
#pred_yt = predict(gbm_op, filter_train %>% select(-ytarget))
#ts.plot(cbind(DATAtest[, c(ytarget)], pred_y), col= c('red','blue'))
#ts.plot(cbind(DATAtrain[, c(ytarget)], pred_yt), col= c('red','blue'))
#ts.plot(sin(2*pi*DATA$DoY / 365.25))
#ts.plot(cos(2*pi*DATA$DoY / 365.25))
#ts.plot(DATA[,ytarget])
#ts.plot(head(DATAtest[,"cos365"],300))
#as.matrix(sparse.model.matrix(~.-1, data = dff))
#class(sparse.model.matrix(~.-1, data = dff))
#class(as.matrix(DATAtrain[, features_x]))
}
if (mname == "elasticNet") {
filter_train <- DATAtrain[, c(ytarget, features_x)] %>%
replace(is.na(.), 0)
filter_test <- DATAtest[, c(ytarget, features_x)] %>%
replace(is.na(.), 0)
#filter_valid <- filter_train %>% tail(20)
#filter_train <- filter_train %>% head(nrow(filter_train)-20)
lambdas <- c(seq(0.1, 0.91, 0.1), seq(1,45, 2) )
alphas <- c(seq(0.8, 1, 0.1), 1)
#length(lambdas)
ts_trControl <- trainControl("timeslice", number= 2,
initialWindow = nrow(filter_train)-20,
skip = 0, fixedWindow = FALSE, horizon = 20, timingSamps = 1,
search = "random")
elastic.net.grid <- expand.grid(list(alpha = alphas, lambda = lambdas) )
set.seed(2)
samp <- sample(1:nrow(elastic.net.grid ), 20)
elastic.net.gridFilter <- elastic.net.grid[samp,]
#init_time1 <- Sys.time()
elastic_net_reg <- train(
x = as.matrix(filter_train %>% select(-ytarget) ),
y = filter_train[,ytarget],
method = "glmnet",
trControl = ts_trControl,
tuneGrid = elastic.net.gridFilter,
metric = 'RMSE',
maximize = FALSE   )
#View(elastic_net_reg$results)
#end_time1 <- Sys.time()
#print(difftime(init_time1,end_time1))
#10*33*6/60
pred <- t(matrix(predict(elastic_net_reg, as.matrix(filter_test %>% select(-ytarget))),
nrow = length(HORIZON[[i.hl]]), ncol= length(seqid), byrow = TRUE))
#ts.plot(pred)
# since 18:00pm to 7:00am
#plot(y = mm[, c(ytarget)][48+9:(24*5)],
#     x =mm[, c("DateTime")][48+9:(24*5)], type = 'l')
#View(mm)
}
if (mname == "AR") {
DATAtrainwow <- DATAtrain[DATAtrain$horizon <= S, ]
DATAtestwow <- (DATAtest[DATAtest$horizon <= S, ] %>% arrange(DateTime))
y <- unlist(DATAtrainwow[, ytarget])
yn <- length(y)
DATAwow <- c(y, unlist(DATAtestwow[, ytarget]))
DATAwow <- zoo::na.locf(DATAwow)
om <- 4*24*7
if (om > length(zoo::na.locf(y))){
om <- (length(zoo::na.locf(y))-1)
}
mod <- ar(zoo::na.locf(y), order.max = om)
pred <- matrix(, length(seqid), H)
for (i.NN in 1:length(seqid)) pred[i.NN, ] <- predict(mod, newdata = DATAwow[yn + (-mod$order + 1):0 + (i.NN - 1) * S], n.ahead = H)$pred
}
if (mname == "ARMA") {
DATAtrainwow <- DATAtrain[DATAtrain$horizon <= S, ]
DATAtestwow <- (DATAtest[DATAtest$horizon <= S, ] %>% arrange(DateTime))
y <- unlist(DATAtrainwow[, ytarget])
yn <- length(y)
DATAwow <- c(y, unlist(DATAtestwow[, ytarget]))
DATAwow <- zoo::na.locf(DATAwow)
om <- 2*24*7
if (om > length(zoo::na.locf(y))){
om <- (length(zoo::na.locf(y))-1)
}
tmp <- rbind(DATAtrainwow, DATAtestwow)
rownames(tmp) <- NULL
mod <- auto.arima(zoo::na.locf(y),
max.p = om, max.q = om, d = 0,
max.P = om, max.Q = om,
seasonal = TRUE, ic = "aic" )
pred <- matrix(, length(seqid), H)
for (i.NN in 1:length(seqid)) pred[i.NN, ] <- predict(mod, newdata = DATAwow[yn + (-mod$order + 1):0 + (i.NN - 1) * S], n.ahead = H)$pred
}
if (mname == "hw") {
cat('estimating model. ........\n')
DATAtrainwow <- DATAtrain[DATAtrain$horizon <= S, ]
DATAtestwow <- (DATAtest[DATAtest$horizon <= S, ] %>% arrange(DateTime))
tmp <- rbind(DATAtrainwow, DATAtestwow)
# View(tmp)
y <- unlist(DATAtrainwow[, ytarget])
yn <- length(y)
DATAwow <- c(y, unlist(DATAtestwow[, ytarget]))
DATAwow <- zoo::na.locf(DATAwow)
# correct outliers and zero values
med_wo_outliers <- median(as.vector(y)[as.vector(y)!=0],na.rm = TRUE)
y[as.vector(y)==0] <- med_wo_outliers
ym <- ts(y,frequency = 24,
start = c(lubridate::year(min(DATAtrainwow$DateTime)),
lubridate::month(min(DATAtrainwow$DateTime)),
as.integer(format(min(DATAtrainwow$DateTime), format = "%d")),
as.integer(format(min(DATAtrainwow$DateTime), format = "%H")),
as.integer(format(min(DATAtrainwow$DateTime), format = "%M")),
as.integer(format(min(DATAtrainwow$DateTime), format = "%S"))))
ym <- ym %>%
replace_na(med_wo_outliers)
mod <- hw(zoo::na.locf(ym), h = H*length(seqid), seasonal= "additive",
lambda= 'auto', damped= FALSE, initial = "optimal",
exponential= FALSE, alpha = NULL, beta= NULL, gamma = NULL)
pred_values_all_days <- predict(mod, n.ahead = H*length(seqid))$mean
pred <- matrix(pred_values_all_days, nrow = length(seqid), ncol =  H)
}
if (mname == "bench") {
ybench <- paste(zone, "_Load_DayAhead", sep = "")
pred <- t(matrix(unlist(DATAtest[, ybench]), nrow = length(HORIZON[[i.hl]]), byrow = TRUE))
}
if (mname == "true") {
pred <- t(matrix(unlist(DATAtest[, ytarget]), nrow = length(HORIZON[[i.hl]]), byrow = TRUE))
}
FORECASTS[seqid, HORIZON[[i.hl]], mname] <- pred
cat(zone, "horizon:", hmin, "-", hmax, " done at split ", round(i.N / Nsplitlen * 100, 2), "% progress, mod:", mname, "\n")
} # i.hl
# cat('process for the horizon **N**finished*******************************************\n',sep = ' ')
} # i.N
end_time <- Sys.time()
cat('....******* model training of', mname,',lasted:...\n',sep = ' ')
print(difftime(init_time,end_time))
ls_train_time[[mname]] <- (difftime(init_time,end_time))
cat('....******* process for the model:', mname, 'finished****.......\n\n',sep = ' ')
} # i.m
ls_train_time
FFT <- FORECASTS
for(i.m in 1:M) FFT[,,i.m]<- FORECASTS[, , "true"]
RES <- FORECASTS - FFT
RMSE <- sqrt(apply(abs(RES)^2, c(3), mean, na.rm = TRUE))
RMSE
MAEh <- apply(abs(RES), c(2,3), mean, na.rm = TRUE)
MAE <- apply(abs(RES), c(3), mean, na.rm = TRUE)
MAE
ts.plot(MAEh[1:200,model.names], col = 1:8, ylab = "MAE")
legend("topleft", model.names, col = 1:8, lwd = 1)
abline(v = 0:10 * S, col = "orange")
abline(v = 0:10 * S - 8, col = "steelblue")
View(elastic_net_reg$results)
plot(tail(ds_edat$DateTime,100),
tail(ds_edat$NL_Load_Actual,100),
type = "l",
col = 'red',
xlab = "Year",
ylab = "Values")
plot(head(mmzz$DateTime,16+(24*1000)),head(mmzz$NL_Load_Actual,16+(24*1000)),type='l')
simple_corr_matrix_plot(DATA %>%
select_if(is.numeric),
0.8 ,0.9 , 'corr using spearman',
colnames(DATA %>%
select_if(is.numeric)),"spearman" )
?trainControl
ls_train_time
View(elastic_net_reg$results)
elastic_net_reg
elastic.net.grid
ts.plot(MAEh[1:200,model.names], col = 1:8, ylab = "MAE")
legend("topleft", model.names, col = 1:8, lwd = 1)
abline(v = 0:10 * S, col = "orange")
abline(v = 0:10 * S - 8, col = "steelblue")
ts.plot(as.vector(FORECASTS[,,'elasticNet'])[1:(24*50)],col='red')
lines(as.vector(FORECASTS[,,'bench'])[1:(24*50)],col='pink')
lines(as.vector(FORECASTS[,,'true'])[1:(24*50)],col='blue')
lines(as.vector(FORECASTS[,,'AR'])[1:(24*50)],col='green')
ts.plot(as.vector(FORECASTS[,,'elasticNet'])[1:(24*5)],col='red')
lines(as.vector(FORECASTS[,,'bench'])[1:(24*5)],col='pink')
lines(as.vector(FORECASTS[,,'true'])[1:(24*5)],col='blue')
lines(as.vector(FORECASTS[,,'AR'])[1:(24*5)],col='green')
ts.plot(MAEh[1:200,model.names], col = 1:8, ylab = "MAE")
legend("topleft", model.names, col = 1:8, lwd = 1)
abline(v = 0:10 * S, col = "orange")
abline(v = 0:10 * S - 8, col = "steelblue")
?sgd
install.packages("sgd")
